{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b92bf2-fc0c-44f7-81f7-2267ebe2d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8032e5-0226-43b1-b5fe-fcb13758251a",
   "metadata": {},
   "source": [
    "### Step 1: Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa42eb3-f1a5-459e-9ffe-389405184d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5182028-0f38-4869-b3e5-8af07da63893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, a, wkly, comp, to, win, fa, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [go, until, jurong, point, crazy, available, o...  \n",
       "1                     [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, a, wkly, comp, to, win, fa, ...  \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam.csv\")  # CSV with 'text' and 'label' columns\n",
    "df['tokens'] = df['text'].apply(clean_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86a574-b231-46dc-95b3-fee44c8c6556",
   "metadata": {},
   "source": [
    "### Step 2: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55a4dc9f-aed3-42cd-914e-b77ea7a6d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cb9fd-7b0e-4355-8d8a-a259dadab595",
   "metadata": {},
   "source": [
    "###  Step 3: Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "584317b7-0719-4053-a1af-b792962816f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "    vocab = set()\n",
    "    for tokens in data:\n",
    "        vocab.update(tokens)\n",
    "    return list(vocab)\n",
    "\n",
    "vocab = build_vocab(X_train)\n",
    "vocab_index = {word: i for i, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb5063-292e-4bf5-9548-e6d7dd8bfbf6",
   "metadata": {},
   "source": [
    "### Step 4: Train Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afe7f13e-e55a-42b2-ae62-90a60fe9ef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesSpamClassifier:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.word_probs = {}\n",
    "        self.class_probs = {}\n",
    "\n",
    "    def _vectorize(self, tokens):\n",
    "        vector = np.zeros(self.vocab_size)\n",
    "        for word in tokens:\n",
    "            if word in vocab_index:\n",
    "                vector[vocab_index[word]] += 1\n",
    "        return vector\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        spam_counts = np.zeros(self.vocab_size)\n",
    "        ham_counts = np.zeros(self.vocab_size)\n",
    "        spam_total = 0\n",
    "        ham_total = 0\n",
    "\n",
    "        spam_docs = 0\n",
    "        ham_docs = 0\n",
    "\n",
    "        for tokens, label in zip(X, y):\n",
    "            vec = self._vectorize(tokens)\n",
    "            if label == \"spam\":\n",
    "                spam_counts += vec\n",
    "                spam_total += sum(vec)\n",
    "                spam_docs += 1\n",
    "            else:\n",
    "                ham_counts += vec\n",
    "                ham_total += sum(vec)\n",
    "                ham_docs += 1\n",
    "\n",
    "        # Laplace smoothing\n",
    "        self.word_probs['spam'] = (spam_counts + 1) / (spam_total + self.vocab_size)\n",
    "        self.word_probs['ham'] = (ham_counts + 1) / (ham_total + self.vocab_size)\n",
    "\n",
    "        # Prior probabilities\n",
    "        total_docs = spam_docs + ham_docs\n",
    "        self.class_probs['spam'] = spam_docs / total_docs\n",
    "        self.class_probs['ham'] = ham_docs / total_docs\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for tokens in X:\n",
    "            vec = self._vectorize(tokens)\n",
    "\n",
    "            # Calculate log-likelihood\n",
    "            log_prob_spam = np.log(self.class_probs['spam']) + np.sum(vec * np.log(self.word_probs['spam']))\n",
    "            log_prob_ham = np.log(self.class_probs['ham']) + np.sum(vec * np.log(self.word_probs['ham']))\n",
    "\n",
    "            predictions.append(\"spam\" if log_prob_spam > log_prob_ham else \"ham\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba411e-0ef2-4a92-b5bb-ee2bc40a3fc0",
   "metadata": {},
   "source": [
    "### train and split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7414f699-fb19-46b6-ad3d-e88b28a5b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "model = NaiveBayesSpamClassifier(vocab)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.mean([y_pred[i] == y_test.iloc[i] for i in range(len(y_test))])\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf7922-c88a-4873-bee3-42162b140785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
